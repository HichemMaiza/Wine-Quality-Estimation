{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi layer Neural Network With Tensorflow \n",
    "### Hichem MAIZA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries  \n",
    "- First, I import tensorflow: a deep learning framework \n",
    "- Second, I import pandas to easily deal with Data frames and csv files \n",
    "- Third, I import matplotlib.pyplot to plot figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import math \n",
    "from tensorflow.python.framework import ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot Encoding Routine :\n",
    "- The one hot encoding routine is another representation of a class: this technique is used in the case of: the outputs are multiple classes.\n",
    "- In our case we have 6 classes from 3 to 8 \n",
    "- An example of one Hot encoding is : \n",
    "\n",
    "  <table align=\"center\">\n",
    "  <tr>\n",
    "    <th>Class</th>\n",
    "    <th>One Hot Encoding</th> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>3</td>\n",
    "    <td>[1,0,0,0,0,0]</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>4</td>\n",
    "    <td>[0,1,0,0,0,0]</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>5</td>\n",
    "    <td>[0,0,1,0,0,0]</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>6</td>\n",
    "    <td>[0,0,0,1,0,0]</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>7</td>\n",
    "    <td>[0,0,0,0,1,0]</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>8 and 9</td>\n",
    "    <td>[0,0,0,0,0,1]</td> \n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "- The illustrated function takes as input a vector of labels and compute a one hot matrix as output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_onehot(y, nbclass):\n",
    "    '''\n",
    "    Creates a Matrix of one hot vector from (Y)\n",
    "    \n",
    "    Arguments:\n",
    "    Y -- input classes, of shape (1, number of examples)\n",
    "    nbclass -- number of Classes\n",
    "    \n",
    "    Returns:\n",
    "    y_train -- Matrix coded in one hot logic\n",
    "    '''\n",
    "    y_train = [] \n",
    "    for i in range(len(y)) :\n",
    "        temp =np.zeros((nbclass))\n",
    "        # the case of 8 or 9 \n",
    "        if (y[i] == 9):\n",
    "            temp[y[i]- 4] = 1\n",
    "        # other cases\n",
    "        else : \n",
    "            temp[y[i]-3] = 1\n",
    "        y_train.append(temp)\n",
    "    return np.array(y_train).T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data\n",
    "- This Lab is accompanied with two data sets one for the training and the second for the test \n",
    "- I read the Data Sets then I put them in a panda data frames so i can manipulate them easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('winequality_White_train.csv', sep= ';')\n",
    "train_test = pd.read_csv('winequality_White_test.csv', sep= ';') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features and Labels \n",
    "- The data sets are composed of features and labels \n",
    "- I put the features in a matrix which i called it X_train \n",
    "- I put the labels in a vector which i called it Y_train \n",
    "- I also did the same with the test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train  = (train_data.as_matrix().T)[0:11,:]\n",
    "X_test = (train_test.as_matrix().T)[0:11,:]\n",
    "Y_train = convert_to_onehot(train_data['quality'], 6)\n",
    "Y_test  = convert_to_onehot(train_test['quality'], 6)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shapes !!\n",
    "\n",
    "- I keep track of shapes, all the times because this is where tensorflow crashes the most \n",
    "- In deep learning in general matrix dimensions are so important so here i verify my matrices shapes \n",
    "- Note that every column in the X_train matrix is a training sample \n",
    "- Note that every column in the Y_train matrix is a training label \n",
    "- The same thing with test data sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples = 4398\n",
      "number of test examples = 500\n",
      "X_train shape: (11, 4398)\n",
      "Y_train shape: (6, 4398)\n",
      "X_test shape: (11, 500)\n",
      "Y_test shape: (6, 500)\n"
     ]
    }
   ],
   "source": [
    "print (\"number of training examples = \" + str(X_train.shape[1]))\n",
    "print (\"number of test examples = \" + str(X_test.shape[1]))\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test.shape)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Vs Mini Batch \n",
    "\n",
    "- After one week of research i finally understood what a mini-batch stands for : \n",
    "    - We can say that a batch is the hole(complete) data set \n",
    "    - And a mini batch is a small portion of it     \n",
    "- Why this mini batch exist ? and why everyone uses it ? \n",
    "    - when we run gradient descent on the whole data set the gradient become too slow to converge \n",
    "    - The solution is to decompose the data set into mini-batches so the gradient can converge faster \n",
    "- ok ! but what size should we use for this batch\n",
    "    - 1st case : batch = data set --> case of batch gradient descent (the usual case)\n",
    "    - 2nd case : batch = one sample--> so we have to iterate all over the exemples (stochastic gradient descent) and its the worst case\n",
    "        ```python\n",
    "            for Sample in All_train_Examples:\n",
    "                Run_Gradient_Descent()\n",
    "        ```  \n",
    "    - 3rd case : batch = Not too small and not too big : it can be 32,64,128,256,512 in general $2^n$, the best choice !! \n",
    "- There's many methods to compute batches, the one I found it interesting  is to compute mini-batches randomly\n",
    "- I implemented it in two steps : \n",
    "    - The first step is to randomly select sample from X_train and the Corresponding Y_train \n",
    "    - The second step is select mini-batches with respect to the bacth size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_batches(X, Y, mini_batch_size = 64, seed = 0) :\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data\n",
    "    Y -- true \"label\" vector\n",
    "    mini_batch_size -- size of the mini-batches\n",
    "    \n",
    "    Returns:\n",
    "    mini_batches -- list of (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(seed)            # to get the same result every time so i can verify my results       \n",
    "    m = X.shape[1]                  # number of training examples\n",
    "    mini_batches = []\n",
    "        \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((Y.shape[0],m))\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.d\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:,k*mini_batch_size:k*mini_batch_size + mini_batch_size ]\n",
    "        mini_batch_Y = shuffled_Y[:,k*mini_batch_size:k*mini_batch_size + mini_batch_size ]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:,num_complete_minibatches*mini_batch_size:]\n",
    "        mini_batch_Y = shuffled_Y[:,num_complete_minibatches*mini_batch_size:]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Place holders \n",
    "- Tensorflow place holders are variable that can change when the program is running  they are not constant variables \n",
    "    - First, I defined X, a place holder for my input features \n",
    "    - Second, I defined Y, a place holder for my output labels \n",
    "    - The function headers are defined for organizing code and make it more visible "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_placeholders(n_x, n_y):\n",
    "    \"\"\"\n",
    "    Creates the placeholders for the tensorflow session.\n",
    "    \n",
    "    Arguments:\n",
    "    n_x -- size of features vector\n",
    "    n_y -- number of classes \n",
    "    \n",
    "    Returns:\n",
    "    X -- placeholder for the data input\n",
    "    Y -- placeholder for the input labels\n",
    "    \"\"\"\n",
    "    X = tf.placeholder(dtype = 'float', shape=[n_x, None], name= 'X')\n",
    "    Y = tf.placeholder(dtype = 'float', shape=[n_y, None], name= 'Y')\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture !! \n",
    "- here I defined my network architecture \n",
    "    - First How many layers will this network have ? \n",
    "        - This is a deep neural network so I chose to  two hidden layers architecture --> so a 3 layer neural network\n",
    "    - Second How many neurons the layer has ? \n",
    "        - It depends on the number of the layer (you can change the number of neurons and see the result)\n",
    "        - In this example i chose a 25 neurons in the 1st layer, 12 in the 2nd, and 6 in the 3rd \n",
    "    - How i defined shapes ? \n",
    "        - I did the calculation by hand, I verify it in hand than i put it into the variables  \n",
    "    - Xavier ? \n",
    "        - After research I found that random initialization is good for weights and zeros initialization for biases to speed up the converging  <br> of the gradient descent, but there are other method more robust for converging, Xavier is one of those methods, the principle is to multiply the random initialization by this Term :  \\begin{equation*}\\frac{1}{\\Bigl(\\sqrt{n^(l-1)})} \\end{equation*}\n",
    "        where l is layer number and n is number of features in the training example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(name = 'variables'):\n",
    "    \n",
    "    \"\"\"\n",
    "    Arguments : \n",
    "    name -- a tensor borad variable name\n",
    "    Returns:\n",
    "    parameters -- a dictionary of tensors containing W1, b1, W2, b2, W3, b3\n",
    "    \"\"\"\n",
    "    \n",
    "    with tf.name_scope(name): \n",
    "        \n",
    "        W1 = tf.get_variable('W1', [25,11], initializer= tf.contrib.layers.xavier_initializer(seed = 1)) \n",
    "        b1 = tf.get_variable('b1', [25,1], initializer= tf.zeros_initializer())\n",
    "        W2 = tf.get_variable('W2', [12,25], initializer= tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "        b2 = tf.get_variable('b2', [12,1], initializer= tf.zeros_initializer())\n",
    "        W3 = tf.get_variable('W3', [6,12], initializer= tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "        b3 = tf.get_variable('b3', [6,1], initializer= tf.zeros_initializer())\n",
    "\n",
    "        parameters = {\"W1\": W1,\n",
    "                      \"b1\": b1,\n",
    "                      \"W2\": W2,\n",
    "                      \"b2\": b2,\n",
    "                      \"W3\": W3,\n",
    "                      \"b3\": b3}\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Graph \n",
    "\n",
    "- Here I defined my tensorflow graph which represents the forward pass of my neural network \n",
    "- Here I chose to work with the Relu activation Function, why ?? \n",
    "    - The Relu activation function is used in cases where the problem is a multi-class classification  \n",
    "    - The Relu activation function is the most used function in neural network \n",
    "- why not sigmoid ? \n",
    "    - Sigmoid activation function is used in the case of logictic regression, and usually when the problem is binary classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters, name = 'forward_pass'):\n",
    "    \"\"\"    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder\n",
    "    parameters -- python dictionary containing \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n",
    "\n",
    "    Returns:\n",
    "    Z3 -- the output of the last LINEAR unit\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve the parameters from the dictionary \"parameters\" \n",
    "    with tf.name_scope(name):\n",
    "        \n",
    "        W1 = parameters['W1']\n",
    "        b1 = parameters['b1']\n",
    "        W2 = parameters['W2']\n",
    "        b2 = parameters['b2']\n",
    "        W3 = parameters['W3']\n",
    "        b3 = parameters['b3']\n",
    "\n",
    "        Z1 = tf.add(tf.matmul(W1, X), b1)                                              \n",
    "        A1 = tf.nn.relu(Z1)                                             \n",
    "        Z2 = tf.add(tf.matmul(W2, A1), b2)\n",
    "        A2 = tf.nn.relu(Z2)                                             \n",
    "        Z3 = tf.add(tf.matmul(W3, A2), b3)                                              \n",
    "    \n",
    "    return Z3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Function \n",
    "- Here I defined my cost function \n",
    "- We chose to compute a cross entropy cost function, why ? \n",
    "    - in cases where we have multi-class classification usually we chose the cross entropy as a cost function \n",
    "    - I used Z3 instead of A3 the output of my Forward pass, why ? \n",
    "        - This is because tensorflow does the final softmax automatically in other words, if  we are to write our own function, we will \n",
    "        do <br>Z3 -- > A3 -- > softmax -- > Cross Entropy, but tensorflow include the softmax function with cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(Z3, Y, name =  'compute_cost'):\n",
    "    \"\"\"\n",
    "    Computes the cost\n",
    "    \n",
    "    Arguments:\n",
    "    Z3 -- output of forward propagation \n",
    "    Y -- \"true\" labels vector placeholder, same shape as Z3\n",
    "    \n",
    "    Returns:\n",
    "    cost - Tensor of the cost function\n",
    "    \"\"\"\n",
    "    with tf.name_scope(name) : \n",
    "        \n",
    "        logits = tf.transpose(Z3)\n",
    "        labels = tf.transpose(Y)\n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = labels))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model !  \n",
    "- Here I defined my model \n",
    "    - First the learning rate: which represents the coefficient when updating the model parameters, it represents one the important hyperparameters \n",
    "        - learning_rate = 0.0001 or learning_rate = 0.001 or learning_rate = 0.01 or learning_rate = 0.1 \n",
    "    - Second the num_epochs : which represent the number times my optimizing process will execute \n",
    "        - num_epochs = 500, num_epochs = 1000, num_epochs = 1500, num_epochs = 2000\n",
    "    - Third the minibatch size: which define the amount of data the algorithm will use at each step of optimization \n",
    "- Run the session : \n",
    "    - As every neural network, it's necessary to compute for steps \n",
    "        - Repeat \n",
    "            - Run Forword Pass\n",
    "            - Compute Cost\n",
    "            - Run Backword Pass : we don't define it because tensor flow run it for us\n",
    "                - Run Backword Pass \n",
    "                - Update parameters         \n",
    "- Compute the accuracy of my system \n",
    "- Compute the confusion matrix\n",
    "- Discuss the result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001,\n",
    "          num_epochs = 1500, minibatch_size = 32, print_cost = True):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X_train -- training set\n",
    "    Y_train -- training set \n",
    "    X_test -- test set \n",
    "    Y_test -- test set\n",
    "    learning_rate -- learning rate of the optimization\n",
    "    num_epochs -- number of epochs of the optimization loop\n",
    "    minibatch_size -- size of a minibatch\n",
    "    print_cost -- True to print the cost every 100 epochs\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    tf.set_random_seed(1)                             # to keep consistent results\n",
    "    seed = 3                                          # to keep consistent results\n",
    "    (n_x, m) = X_train.shape                          # (n_x: input size, m : number of examples in the train set)\n",
    "    n_y = Y_train.shape[0]                            # n_y : output size\n",
    "    costs = []                                        # To keep track of the cost\n",
    "    \n",
    "    # Create Placeholders of shape (n_x, n_y)\n",
    "    X, Y = create_placeholders(n_x, n_y)\n",
    "\n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters()\n",
    "    \n",
    "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
    "    Z3 = forward_propagation(X, parameters)\n",
    "    \n",
    "    # Cost function: Add cost function to tensorflow graph\n",
    "    cost = compute_cost(Z3, Y)\n",
    "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.\n",
    "    with tf.name_scope('train'): \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)    \n",
    "    \n",
    "    # Initialize all the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Run the initialization\n",
    "        sess.run(init)\n",
    "        \n",
    "        # Do the training loop\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            epoch_cost = 0.                       # Defines a cost related to an epoch\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            seed = seed + 1\n",
    "            minibatches = random_batches(X_train, Y_train, minibatch_size, seed)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "\n",
    "                # Select a minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                \n",
    "                # IMPORTANT: The line that runs the graph on a minibatch.\n",
    "                # Run the session to execute the \"optimizer\" and the \"cost\", the feedict should contain a minibatch for (X,Y).\n",
    "                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n",
    "                \n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "\n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 100 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "                \n",
    "        # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "        # lets save the parameters in a variable\n",
    "        parameters = sess.run(parameters)\n",
    "        print (\"Parameters have been trained!\")\n",
    "\n",
    "        # Calculate the correct predictions\n",
    "        correct_prediction = tf.equal(tf.argmax(Z3,0), tf.argmax(Y,0))\n",
    "        \n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        \n",
    "        print (\"Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test}))\n",
    "        \n",
    "        #Z3 = forward_propagation(X, parameters)\n",
    "        pred = tf.argmax(Z3,0)\n",
    "        original = tf.argmax(Y,0)\n",
    "        prediction = sess.run(Z3, feed_dict = {X: X_test})\n",
    "        \n",
    "        cm = tf.confusion_matrix(original, pred)\n",
    "        print (\"\\n the confusion matrix is \\n\", sess.run(cm, feed_dict = {X: X_test, Y :Y_test }))\n",
    "        \n",
    "        # tensor board \n",
    "        writer = tf.summary.FileWriter('./train') \n",
    "        writer.add_graph(sess.graph)\n",
    "\n",
    "                \n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 7.548606\n",
      "Cost after epoch 100: 1.269766\n",
      "Cost after epoch 200: 1.197444\n",
      "Cost after epoch 300: 1.155308\n",
      "Cost after epoch 400: 1.129607\n",
      "Cost after epoch 500: 1.111189\n",
      "Cost after epoch 600: 1.100523\n",
      "Cost after epoch 700: 1.093844\n",
      "Cost after epoch 800: 1.089890\n",
      "Cost after epoch 900: 1.086204\n",
      "Cost after epoch 1000: 1.082942\n",
      "Cost after epoch 1100: 1.084028\n",
      "Cost after epoch 1200: 1.082413\n",
      "Cost after epoch 1300: 1.080022\n",
      "Cost after epoch 1400: 1.078016\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEWCAYAAABliCz2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxwzwgaYqRwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xcb6d1c29e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters have been trained!\n",
      "Test Accuracy: 0.57\n",
      "\n",
      " the confusion matrix is \n",
      " [[  1   0   0   0   0   0]\n",
      " [  0   2   5   3   0   0]\n",
      " [  0   1  61  57   4   0]\n",
      " [  0   1  36 193  52   0]\n",
      " [  0   0   2  49  28   0]\n",
      " [  0   0   0   2   3   0]]\n"
     ]
    }
   ],
   "source": [
    "parameters = model(X_train, Y_train, X_test, Y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Discussion \n",
    "- I made a flexible model so i can modify any of its parameters (learning rate, batch size, num of epoch )\n",
    "- The model has an accuracy of 0.58 (last configuration)\n",
    "- From the confusion matrix I can conclude that my model didn't detect properly the 8 th class --> 0 true positive detection \n",
    "- the class 6 has a 193 true positive detected points \n",
    "- my cost function is decreasing so the model is optimizing the cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
